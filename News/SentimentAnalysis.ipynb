{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing the pandas package\n",
    "import pandas as pd\n",
    "\n",
    "#reading the dataset into dataframe\n",
    "train = pd.read_csv(\"../input/labeledTrainData.tsv\",header=0, sep='\\t', quoting=3)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "4af26cdf6fab48f80ff7282425603beebca9e0c7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train[\"review\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "3348df650fd9adc82da00fd66e79c653c53b1d59",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To remove HTML Markup, we'll use Beautiful Soup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Initializing BS on a single Movie Review Object\n",
    "rev1 = BeautifulSoup(train[\"review\"][0],\"lxml\")\n",
    "\n",
    "#Printing the raw review and corrected text for cmp :\n",
    "print(train[\"review\"][0])\n",
    "print(\"\\n-------- HTML MARKUP REMOVED ---------\\n\")\n",
    "print(rev1.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "a45b0b2a2ba5e928573dcd7aeef0c05a0e4543cc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To tackle numbers and punctuation : re {Regular Expression} library is used\n",
    "import re\n",
    "letters_only = re.sub(\"[^a-zA-Z]\",\" \", rev1.get_text())\n",
    "print(letters_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "fe089786dcf23c4d8f86f61a4268803790f874e3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting text to lowercase\n",
    "lower_case = letters_only.lower()\n",
    "\n",
    "#Tokenizing or Splitting the Words\n",
    "words = lower_case.split()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "scrolled": true,
    "_uuid": "3f2b55fac075204bd5147d346ee1852f23815a27",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stop words : Words with negligible or no meaning\n",
    "#To remove Stop words, we'll use NLTK package by downloading the stop words library\n",
    "import nltk\n",
    "#nltk.download()\n",
    "#using nltk to fetch and remove the list of stop words\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))\n",
    "\n",
    "#Remove these words from the tokenized set\n",
    "words = [w for w in words if not w in stopwords.words(\"english\")]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "6f3b6117146c8cc62b02eff83186f488d706ca77"
   },
   "outputs": [],
   "source": [
    "#Review Cleaning function : \n",
    "def rev_to_words(raw_review):\n",
    "    #Bye-Bye HTML Markups !\n",
    "    review_text = BeautifulSoup(raw_review,\"lxml\").get_text()\n",
    "    #Bye-Bye Non-Letters !\n",
    "    alpha_only = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #GetLow\n",
    "    words = alpha_only.lower().split()\n",
    "    #Optimizing is the Goal, List to Set of stopwords for faster Search\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    #Bye-Bye Stop Words, Thanks for Stopping By !\n",
    "    final_words = [w for w in words if not w in stops]\n",
    "    #Fusing the words into a single space separated string\n",
    "    return( \" \".join(final_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "20baa56f182aa2c5206d78807bf2ea3dab284382",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_review = rev_to_words(train[\"review\"][0])\n",
    "print(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "e0742327a72fb9a8715f3e840f8cc1f9c94c1b0e"
   },
   "outputs": [],
   "source": [
    "#Cleaning the entire training set\n",
    "\n",
    "#Getting the Number of Reviews\n",
    "num_rev = train[\"review\"].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "1124daf9d13ccf97fd4ac4fc1d49aad99eabb583",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Empty list to hold the cleaned reviews\n",
    "clean_train_reviews = []\n",
    "\n",
    "#Loop through the range of reviews\n",
    "print(\"Cleaning and parsing the training set movie reviews\")\n",
    "for i in range(0, num_rev):\n",
    "    if((i+1)%1000==0):\n",
    "        print(\"Review\",(i+1),\"/\",num_rev,\"\\n\")\n",
    "    clean_train_reviews.append(rev_to_words(train[\"review\"][i]))\n",
    "\n",
    "print(\"Extracting features and creating bag of words\\n\")\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 5000)\n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "#Converting to arrays for easier manipulation\n",
    "train_data_features = train_data_features.toarray()\n",
    "print(train_data_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "4828a705470275840a26cc3068e026f779fccf65",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will check the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "21bc485c0f796b30baae7dafb408c28b878881c3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Summing up the counts of each vocab word\n",
    "import numpy as np\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "2e999f6e9047da59841de306192ad9b105070c85",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Associating each word with its count by zipping it\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print(tag,\" : \",count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "267809a89d59034c2505f466955df45da4f4cb8d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using Random Forest for Supervised learning \n",
    "print(\"Random Forest being trained...\")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Intitializing with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "#Fitting the forest to the training data\n",
    "forest = forest.fit(train_data_features, train[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "6bf8d3c68adfab0bf1b906faf95473867a5dcb5b",
    "scrolled": true,
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Training Successful.\")\n",
    "#Testing the Trained Model over TestData.tsv\n",
    "test = pd.read_csv(\"../input/testData.tsv\",sep='\\t',header=0,quoting=3)\n",
    "\n",
    "#Verifying that there are correct number of rows and columns\n",
    "print(test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "c95397f0d8c77fc93987e7e5e5b80aedd32fdd94"
   },
   "outputs": [],
   "source": [
    "#Creating an empty list for appending the clean reviews\n",
    "num_reviews = len(test[\"review\"])\n",
    "clean_test_reviews = []\n",
    "\n",
    "print(\"Cleaning and parsing the training set movie reviews\")\n",
    "for i in range(0, num_rev):\n",
    "    if((i+1)%1000==0):\n",
    "        print(\"Review\",(i+1),\"/\",num_rev,\"\\n\")\n",
    "    clean_review = rev_to_words(test[\"review\"][i])\n",
    "    clean_test_reviews.append(clean_review)\n",
    "\n",
    "print(\"Extracting features and creating bag of words\\n\")\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "test_data_features = test_data_features.toarray()\n",
    "\n",
    "#Using the trained Random Forest for predicting sentiments : \n",
    "result = forest.predict(test_data_features)\n",
    "\n",
    "#Copying the results to a pandas dataframe with [\"ID\",\"Sentiment\"]\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"],\"sentiment\":result})\n",
    "\n",
    "#Using pandas to write the output\n",
    "output.to_csv(\"Bag_of_Words_model.csv\", index=False, quoting=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
